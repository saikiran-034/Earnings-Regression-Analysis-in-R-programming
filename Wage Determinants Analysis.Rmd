---
title: "Wage Determinants Analysis: A Regression and Exploratory Study on Young and Old Workers"
author: "Mada Sai Kiran"

output:
  pdf_document:
    number_sections: true #This gives automatic numbers in the PDF document 
    keep_tex: false # 'keep_tex' to false to not keep the intermediate .tex file after knitting.
    latex_engine: xelatex # Using 'xelatex' as the LaTeX engine for compiling the PDF
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE) # echo = false, because not to display the code in the final report, same as messages, and warnings
```

```{r}

if (!require("ggplot2")) install.packages("ggplot2") #only install the package if our machine don't have the package
library(ggplot2) #importing the library
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
if (!require("knitr")) install.packages("knitr") 
library(knitr) 
if (!require("broom")) install.packages("broom") 
library(broom) 
```

```{r}
df <- read.csv("Wage data.csv")      #Reading csv file
```

# Data Exploration

```{r, include = TRUE}
summary(df[, c("age", "educ", "gender", "hrswork", "wage", "nchild")])  #statistics of age, education, gender, hrswork, wage, nchild

```

The dataset contains a variety of variables related to individuals, including age, education level, gender, hours worked per week, wage, and number of children. These summary statistics provide a first look at the distribution of each variable and reveal that wages and hours worked have a wide range of values.

```{r plots, fig.height=3, fig.width=2.4, include = TRUE}
#giving figure sizes like height and width
hist(df$age, main="Age", xlab="Age") #plotting histograms
hist(df$wage, main="Wage", xlab="Wage")
hist(df$hrswork, main="Hours Worked", xlab="Hours")
```

From the histograms we got the information like most people earn lower wages, but only a few people make significantly more.Similarly, while most people work a standard 40 hours a week, there are some people who put in much longer hours. Age seems to be more evenly distributed across the sample.

```{r fig.height=3.5, fig.width=6, include = TRUE}
barplot(table(df$gender), main="Gender", col = c("lightcyan", "lightgoldenrod")) #plotting Barplots
barplot(table(df$race), main="Race", col = c("moccasin", "Black", "lavender"))
barplot(table(df$marital), main="Marital Status",col = c("lightblue", "lightpink", "palegreen"))
```

* Above bar plots show that
  + There are more males when compared to females.
  + Larger number of individuals belong to White race group among all. 
  + Marital status distribution shows that a significant proportion are married.


```{r, include = TRUE}
corr_matrix <- cor(df[, c("age", "hrswork", "wage", "nchild")]) #finding correlation between age, hrswork, wage, nchild and storing into corr_matrix
corr_df <- as.data.frame(corr_matrix) # Converting the correlation matrix into data frame
kable(corr_df, caption = "Correlation Matrix of Selected Variables") # Displaying the correlation matrix as a table by using kable function 
```
This correlation matrix helps us to know that the relationship between hours work and wage is positively correlation. Which mean if individuals work more they will get more Wages. Additionally, relationship between age and wage is weaker correlation which mean there is no that much relation between both elements.

# Probability & Confidence Intervals

```{r}
p_no_insure <- mean(df$insure == 0, na.rm = TRUE) #mean of people with no private insurance
p_at_least_one <- 1 - (1 - p_no_insure)^5 #probability of at least one person out of five randomly selected people don't have private insurance
```

As per the given data set, Approximately `r round(p_no_insure * 100, 1)`% of individuals are not covered by private insurance. This gives a probability of `r round(p_at_least_one, 3)` that at least one out of five randomly selected individuals lacks private insurance.

```{r}
p_nchild_given_married <- mean(df[df$marital == 1, ]$nchild >= 1, na.rm = TRUE) #percentage of married person with having atlease one child
```
Among married individuals, `r round(p_nchild_given_married * 100, 1)`% have at least one child.

```{r, include=TRUE}
children_freq <- table(df$nchild)
children_freq_df <- as.data.frame(children_freq)
colnames(children_freq_df) <- c("Number_of_Children", "Frequency")
kable(children_freq_df, caption = "Frequency of Number of Children") 
```
This is the table for sum of number children per individual as per the given data set 
```{r}
nchild_mean <- mean(df$nchild, na.rm = TRUE) #table mean
nchild_var <- var(df$nchild, na.rm = TRUE) #variance of table
p_nchild_ge3 <- mean(df$nchild >= 3, na.rm = TRUE) #probability that people having 3 or more than 3 childrens
```
The mean number of children in a household is `r round(nchild_mean, 2)`, with a variance of `r round(nchild_var, 2)`. The probability that someone has three or more children is `r round(p_nchild_ge3 * 100, 1)`%.

# Estimates & Hypothesis Tests

```{r, include = TRUE}
children2 <- df[df$nchild == 2, ]
t.test(children2$wage)  # one-sample t-test on the wages of individuals with 2 children

children5 <- df[df$nchild >= 5, ]
if(nrow(children5) > 1) {
  t.test(children5$wage) # one-sample t-test on the wages if there is more than 1 observation with 5 childrens or more
} else {
  "Too few observations for 95% CI" # Returns a message if sample size is too small for a confidence interval
}

insurance_gender_table <- table(df$gender, df$insure)
chisq.test(insurance_gender_table) # Chi-squared test to check for independence between gender and insurance coverage
```
 The mean wage for people with two children amounts to approximately `r  round(mean(children2$wage, na.rm = TRUE), 2)`. The confidence interval shows an estimated range for the  actual population mean.

The mean wage for people with 5 children or above amounts to approximately `r  round(mean(children5$wage, na.rm = TRUE), 2)`. The confidence interval shows an estimated range for the  actual population mean. We have only 11 individuals having children 5 or more in the given data set.

I evaluated chi-squared test for the connection between gender and insurance  status. So, p-value determines whether we should accept or reject the statistical independence between the two  variables.

# Simple Linear Regression

```{r, include=FALSE}
df$logwage <- log(df$wage)
young <- df[df$age < 35, ] # young age individual if the age is below 35
old <- df[df$age >= 35, ] # old age people if the age is 35 or more

model_young <- lm(logwage ~ age, data = young)  # linear regression model is used to determine the relationship between age and log-transformed wages. This  model is based on the ‘young’ dataset which includes younger people.
model_old <- lm(logwage ~ age, data = old)
summary(model_young)
summary(model_old)
```

To know how age affects wages, I first applied a log transformation to the wage data. Then I split age into young(<35) and old(>=35) age groups. Then I applied the linear regression model to see how age affects log-transformed wages within age groups.

* Young age group: The Model shows that there are slight changes in increasing wages as age increases, so younger people gain early career experience.

* Old age group: This showed effect of age on wages is smaller or even flat, it suggests that wage growth or stabilizes later in life.

```{r Fig, fig.height=3.5, fig.width=6, include = TRUE}

plot(young$age, young$logwage, main="Young", xlab="Age", ylab="log(Wage)", col = "Magenta")  #plotting age against log transformed wages to visually explore their relationship for individuals in  the 'young' data set.
abline(model_young, col="blue") #Adding the regression line from the model to the plot in blue to show the trend between age and log-transformed wages.


plot(old$age, old$logwage, main="Old", xlab="Age", ylab="log(Wage)", col="cyan")
abline(model_old, col="red")
```

I visualized scatter plots for better understanding of young age group and old age group effects log(wage)

* Here we can clearly see that in young scatter plot Blue line is increasing so as age increases wage also increasing
* where as in old scatter plot Red line is almost flat or it is slight increasing so life is stabilizing

# Multiple Linear Regression

```{r, include = TRUE}

categoricals <- c("gender", "race", "marital", "region", "educ")  #taking multiple categorical values to convert into factors 
df[categoricals] <- lapply(df[categoricals], factor) # Converting categorical variables to factors with lapply function
model_full_young <- lm(logwage ~ . -wage, data = young) 
model_full_old <- lm(logwage ~ . -wage, data = old)
summary_young <- tidy(model_full_young) #applying tidy function on young linear regression model 
summary_old <- tidy(model_full_old)
kable(summary_young, caption = "Summary of the Young Model")
kable(summary_old, caption = "Summary of the Old Model")

```

To explore what factors effects wages differently for younger and older individuals, I built two separate multiple linear regression models — one for people under 35 (young) and another for those aged 35 and above (old). In both models, I used log(wage) as the response variable, with all other available variables (except wage) as predictors. These are referred to as the full models.

Before fitting the models, I converted the categorical variables — gender, race, marital status, region, and education — into factors. Because, this step ensures R handles them properly in the regression analysis.

* Comparing the Models

When comparing these full models to the simple linear regression models, I found that the full models had higher R-squared values. This means they do a better job explaining the variation in wages because they consider multiple factors rather than just one.

This Full models also showed that the impact of different variables on wages changes with age. For example, education or marital status might play a bigger role in one group than the other, highlighting how wage determinants can shift across different life stages.

* Why Not Always Use the Full Model?

Even though the full models fit the data better, including all variables isn't always the good idea. When we add too many predictors, then model try to capture noise rather than the patterns. Here we it can leads to risk of overfitting. This make the model not to recognize properly.

That’s why it can be useful to build a reduced model, keeping only the most meaningful variables. This can be done using selection techniques like AIC, BIC, or even just logical reasoning based on what makes sense. A simple model is usually easier to understand and interpret, and often performs better in the long run.

